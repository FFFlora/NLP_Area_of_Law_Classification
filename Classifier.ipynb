{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n1. predict the unlabeled data (Naive Bayes, SVM)\\n2. evaluate the accuracy,f1,ruc,etc?? (check with the doc later)\\n3. Topic Modeling\\n4. wordcloud(maybe,if applicable)\\n5. visualization(maybe, if applicable)\\n6. think about some other fun questions about this data set\\n7. paper \\n    Exploring the Use of Text Classification in the Legal Domain - arXiv\\n    https://arxiv.org/pdf/1710.09306\\n    \\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TODO\n",
    "\n",
    "\"\"\"\n",
    "1. predict the unlabeled data (Naive Bayes, SVM)\n",
    "2. evaluate the accuracy,f1,ruc,etc?? (check with the doc later)\n",
    "3. Topic Modeling\n",
    "4. wordcloud(maybe,if applicable)\n",
    "5. visualization(maybe, if applicable)\n",
    "6. think about some other fun questions about this data set\n",
    "7. paper \n",
    "    Exploring the Use of Text Classification in the Legal Domain - arXiv\n",
    "    https://arxiv.org/pdf/1710.09306\n",
    "    \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string \n",
    "import nltk\n",
    "import re \n",
    "import numpy as np\n",
    "#settings\n",
    "pd.options.display.max_rows=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Judgements</th>\n",
       "      <th>Area.of.Law</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LNIND_1988_CAL_114</td>\n",
       "      <td>To be Tested</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LNIND_1956_CAL_163</td>\n",
       "      <td>To be Tested</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LNIND_1976_CAL_277</td>\n",
       "      <td>To be Tested</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LNIND_1980_CAL_52</td>\n",
       "      <td>To be Tested</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LNIND_1955_CAL_124</td>\n",
       "      <td>To be Tested</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Judgements   Area.of.Law\n",
       "0  LNIND_1988_CAL_114  To be Tested\n",
       "1  LNIND_1956_CAL_163  To be Tested\n",
       "2  LNIND_1976_CAL_277  To be Tested\n",
       "3   LNIND_1980_CAL_52  To be Tested\n",
       "4  LNIND_1955_CAL_124  To be Tested"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapping = pd.read_csv('data/Interview_Mapping.csv')\n",
    "mapping.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create labels\n",
    "unlabeled = []\n",
    "labeled = []\n",
    "labels = []\n",
    "\n",
    "for index,row in mapping.iterrows():\n",
    "    if row['Area.of.Law'] == 'To be Tested':\n",
    "        unlabeled.append(row['Judgements'])\n",
    "    else: \n",
    "        labeled.append(row['Judgements'])\n",
    "        labels.append(row['Area.of.Law'])\n",
    "        \n",
    "# how much unique area of law        \n",
    "len(set(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load files\n",
    "import os\n",
    "\n",
    "unlabeled_text=[]\n",
    "labeled_text=[]\n",
    "\n",
    "for name in unlabeled:\n",
    "    path = os.path.join('data/',name+'.txt')\n",
    "    with open(path,'r',errors = 'ignore') as f:\n",
    "        unlabeled_text.append(f.read())\n",
    "for name in labeled:\n",
    "    path = os.path.join('data/',name+'.txt')\n",
    "    with open(path,'r',errors = 'ignore') as f:\n",
    "        labeled_text.append(f.read())\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "labeled_text[] each of the element in this array is a passage\n",
    "and what i need to do is to clean each of the passage first \n",
    "and combine the cleaned passages together into a new array "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "punct = string.punctuation\n",
    "\n",
    "labeled_cleaned=[]\n",
    "unlabeled_cleaned = []\n",
    "\n",
    "for passage in labeled_text:\n",
    "        \n",
    "    # remove links \n",
    "    #passage= re.sub(r'http(s)?:\\/\\/\\S*', \"\", str(passage))\n",
    "    #passage = ''.join([elem if elem.isalnum() or elem.isspace() else \" \" for elem in passage ])\n",
    "    # remove \\n\n",
    "    passage = ''.join([elem.strip('\\n') for elem in passage])\n",
    "    # normalization and remove stopwords\n",
    "    passage = ' '.join([elem for elem in passage.lower().split() if elem not in stop])\n",
    "    #remove punctuation \n",
    "    passage = ''.join([elem.replace('[^\\w\\s]','') for elem in passage])\n",
    "    #remove digits\n",
    "    #passage = ''.join([elem for elem in passage if not elem.isdigit()])\n",
    "    \n",
    "    # ??? common word\n",
    "    #freq = pd.Series(passage.split()).value_counts()[:10]\n",
    "    \n",
    "    #lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    passage = ' '.join(lemmatizer.lemmatize(elem) for elem in passage.split())\n",
    "    #passage = passage.lower().split()\n",
    "    labeled_cleaned.append(passage)\n",
    "\n",
    "    \n",
    "for passage in unlabeled_text:\n",
    "        \n",
    "    # remove links \n",
    "   # passage= re.sub(r'http(s)?:\\/\\/\\S*', \"\", str(passage))\n",
    "  #  passage = ''.join([elem if elem.isalnum() or elem.isspace() else \" \" for elem in passage ])\n",
    "    # remove \\n\n",
    "    passage = ''.join([elem.strip('\\n') for elem in passage])\n",
    "    # normalization and remove stopwords\n",
    "    passage = ' '.join([elem for elem in passage.lower().split() if elem not in stop])\n",
    "    #remove punctuation \n",
    "    passage = ''.join([elem.replace('[^\\w\\s]','') for elem in passage])\n",
    "    #remove digits\n",
    "    #passage = ''.join([elem for elem in passage if not elem.isdigit()])\n",
    "    \n",
    "    # ??? common word\n",
    "    #freq = pd.Series(passage.split()).value_counts()[:10]\n",
    "    \n",
    "    #lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    passage = ' '.join(lemmatizer.lemmatize(elem) for elem in passage.split())\n",
    "    #passage = passage.lower().split()\n",
    "    unlabeled_cleaned.append(passage)\n",
    "    # it has to be a string so it could be processed later \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# split the labeled data into training and validation set \n",
    "# use 7-3 \n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "train_text,val_text,train_labels,val_labels =train_test_split(labeled_cleaned,labels,test_size = 0.3,random_state = 0)\n",
    "\n",
    "# do tfidf to get X_train and X_val\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train = vectorizer.fit_transform(train_text)\n",
    "X_val = vectorizer.transform(val_text)\n",
    "\n",
    "# do label encoding to get y_train and y_val\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(labels)\n",
    "y_train = encoder.transform(train_labels)\n",
    "y_val = encoder.transform(val_labels)\n",
    "all_labels = encoder.classes_\n",
    "\n",
    "# do tfidf to get X_test (unlabeled text that needed to be predicted)\n",
    "X_test = vectorizer.transform(unlabeled_cleaned) #transform on test set, not fit_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score:  0.5592592592592592\n",
      "[34  6  6 14 14  6  6 28 23  6 23  6 23 39  6 14  6 39 37  6 23 23 37  6\n",
      " 39  6 23  6 39 23  6 39  6 23 34 14  1  6 14 15  6  6 13 23  6 34 14  6\n",
      " 39 14 13 39 39  6  1 28  1 23  6  6 23  6 39 13 34  6  6  6  6 14  6 23\n",
      "  6  6 37  6 23 14  6 14  6  7  6 39  6 39 34 37 23 23 39 23 23 14  6  6\n",
      " 23 38 13  6]\n",
      "Train accuracy :  0.7313195548489666\n",
      "Test accuracy :   0.5592592592592592\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score:  0.31851851851851853\n",
      "[ 6  6  6  6  6  6  6  6 23  6 23  6 23 39  6  6  6 39  6  6 23 23  6  6\n",
      " 39  6 23  6 39 23  6 39  6 23  6 14  6  6  6  6  6  6  6 23  6  6  6  6\n",
      " 39  6  6 39 39  6  6  6  6  6  6  6 23  6  6  6  6  6  6  6  6  6  6 23\n",
      "  6  6  6  6 23  6  6  6  6  6  6 39  6  6  6  6  6 23 39 23 23  6  6  6\n",
      " 23  6  6  6]\n",
      "Train accuracy :  0.36089030206677264\n",
      "Test accuracy :   0.31851851851851853\n"
     ]
    }
   ],
   "source": [
    "# use Naive Bayes\n",
    "from sklearn import naive_bayes\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "modelNB = naive_bayes.MultinomialNB()\n",
    "modelNB.fit(X_train,y_train)\n",
    "predicted_labels = modelNB.predict(X_val)\n",
    "print('f1 score: ',f1_score(y_val,predicted_labels,average='micro'))\n",
    "result = modelNB.predict(X_test)\n",
    "print(result)\n",
    "\n",
    "yHat_train = modelNB.predict(X_train)\n",
    "yHat_val = modelNB.predict(X_val)\n",
    "\n",
    "print(\"Train accuracy : \", accuracy_score(y_train, yHat_train))\n",
    "print(\"Test accuracy :  \", accuracy_score(y_val, yHat_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score:  0.2814814814814815\n",
      "[ 6  6  6  6  6  6  6  6 23  6  6  6 23  6  6  6  6  6  6  6 23 23  6  6\n",
      "  6  6 23  6  6  6  6  6  6 23  6  6  6  6  6  6  6  6  6 23  6  6  6  6\n",
      "  6  6  6  6 39  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6\n",
      "  6  6  6  6 23  6  6  6  6  6  6  6  6  6  6  6  6 23  6 23 23  6  6  6\n",
      " 23  6  6  6]\n",
      "Train accuracy :  0.26232114467408585\n",
      "Test accuracy :   0.2814814814814815\n"
     ]
    }
   ],
   "source": [
    "# use SVM\n",
    "from sklearn import svm\n",
    "\n",
    "modelSVM = svm.SVC(kernel='linear',C=0.1)\n",
    "modelSVM.fit(X_train,y_train)\n",
    "predicted_labels = modelSVM.predict(X_val)\n",
    "print('f1 score: ',f1_score(y_val,predicted_labels,average='micro'))\n",
    "result = modelSVM.predict(X_test)\n",
    "print(result)\n",
    "\n",
    "yHat_train = modelSVM.predict(X_train)\n",
    "yHat_val = modelSVM.predict(X_val)\n",
    "\n",
    "print(\"Train accuracy : \", accuracy_score(y_train, yHat_train))\n",
    "print(\"Test accuracy :  \", accuracy_score(y_val, yHat_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score:  0.5592592592592592\n",
      "[34  6  6 14 14  6  6 28 23  6 23  6 23 39  6 14  6 39 37  6 23 23 37  6\n",
      " 39  6 23  6 39 23  6 39  6 23 34 14  1  6 14 15  6  6 13 23  6 34 14  6\n",
      " 39 14 13 39 39  6  1 28  1 23  6  6 23  6 39 13 34  6  6  6  6 14  6 23\n",
      "  6  6 37  6 23 14  6 14  6  7  6 39  6 39 34 37 23 23 39 23 23 14  6  6\n",
      " 23 38 13  6]\n",
      "Train accuracy :  0.7313195548489666\n",
      "Test accuracy :   0.5592592592592592\n"
     ]
    }
   ],
   "source": [
    "# use Linear Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "modelLR = LogisticRegression()\n",
    "modelLR.fit(X_train,y_train)\n",
    "predicted_labels = modelLR.predict(X_val)\n",
    "print('f1 score: ',f1_score(y_val,predicted_labels,average='micro'))\n",
    "result = modelLR.predict(X_test)\n",
    "print(result)\n",
    "yHat_train = modelLR.predict(X_train)\n",
    "yHat_val = modelLR.predict(X_val)\n",
    "\n",
    "print(\"Train accuracy : \", accuracy_score(y_train, yHat_train))\n",
    "print(\"Test accuracy :  \", accuracy_score(y_val, yHat_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Property Laws', 'Civil Procedure', 'Civil Procedure',\n",
       "       'Criminal Procedure', 'Criminal Procedure', 'Civil Procedure',\n",
       "       'Civil Procedure', 'Local Government', 'Income Tax',\n",
       "       'Civil Procedure', 'Income Tax', 'Civil Procedure', 'Income Tax',\n",
       "       'Tenancy Laws', 'Civil Procedure', 'Criminal Procedure',\n",
       "       'Civil Procedure', 'Tenancy Laws', 'Service Law',\n",
       "       'Civil Procedure', 'Income Tax', 'Income Tax', 'Service Law',\n",
       "       'Civil Procedure', 'Tenancy Laws', 'Civil Procedure', 'Income Tax',\n",
       "       'Civil Procedure', 'Tenancy Laws', 'Income Tax', 'Civil Procedure',\n",
       "       'Tenancy Laws', 'Civil Procedure', 'Income Tax', 'Property Laws',\n",
       "       'Criminal Procedure', 'Alternative Dispute Resolution',\n",
       "       'Civil Procedure', 'Criminal Procedure', 'Customs',\n",
       "       'Civil Procedure', 'Civil Procedure', 'Criminal Laws',\n",
       "       'Income Tax', 'Civil Procedure', 'Property Laws',\n",
       "       'Criminal Procedure', 'Civil Procedure', 'Tenancy Laws',\n",
       "       'Criminal Procedure', 'Criminal Laws', 'Tenancy Laws',\n",
       "       'Tenancy Laws', 'Civil Procedure',\n",
       "       'Alternative Dispute Resolution', 'Local Government',\n",
       "       'Alternative Dispute Resolution', 'Income Tax', 'Civil Procedure',\n",
       "       'Civil Procedure', 'Income Tax', 'Civil Procedure', 'Tenancy Laws',\n",
       "       'Criminal Laws', 'Property Laws', 'Civil Procedure',\n",
       "       'Civil Procedure', 'Civil Procedure', 'Civil Procedure',\n",
       "       'Criminal Procedure', 'Civil Procedure', 'Income Tax',\n",
       "       'Civil Procedure', 'Civil Procedure', 'Service Law',\n",
       "       'Civil Procedure', 'Income Tax', 'Criminal Procedure',\n",
       "       'Civil Procedure', 'Criminal Procedure', 'Civil Procedure',\n",
       "       'Company Law', 'Civil Procedure', 'Tenancy Laws',\n",
       "       'Civil Procedure', 'Tenancy Laws', 'Property Laws', 'Service Law',\n",
       "       'Income Tax', 'Income Tax', 'Tenancy Laws', 'Income Tax',\n",
       "       'Income Tax', 'Criminal Procedure', 'Civil Procedure',\n",
       "       'Civil Procedure', 'Income Tax', 'Succession Laws',\n",
       "       'Criminal Laws', 'Civil Procedure'], dtype='<U32')"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = all_labels[result]\n",
    "a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write result in csv\n",
    "with open('predictions.csv','w') as f:\n",
    "    f.write('Judgements' + '\\t' + 'Area of Law' + '\\n')\n",
    "    predictionList = all_labels[result]\n",
    "    for i in range(0, len(result)):\n",
    "        f.write(unlabeled[i] + '\\t' + predictionList[i] + '\\n')\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     Judgements\\tArea of Law\n",
      "0        LNIND_1988_CAL_114\\tProperty Laws\\t\n",
      "1      LNIND_1956_CAL_163\\tCivil Procedure\\t\n",
      "2      LNIND_1976_CAL_277\\tCivil Procedure\\t\n",
      "3    LNIND_1980_CAL_52\\tCriminal Procedure\\t\n",
      "4   LNIND_1955_CAL_124\\tCriminal Procedure\\t\n",
      "..                                       ...\n",
      "95     LNIND_1980_CAL_279\\tCivil Procedure\\t\n",
      "96          LNIND_1980_CAL_229\\tIncome Tax\\t\n",
      "97     LNIND_1988_CAL_232\\tSuccession Laws\\t\n",
      "98       LNIND_1957_CAL_142\\tCriminal Laws\\t\n",
      "99     LNIND_1988_CAL_107\\tCivil Procedure\\t\n",
      "\n",
      "[100 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "sss = pd.read_csv('predictions.csv')\n",
    "print(sss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\taaa\tbbb\t\n"
     ]
    }
   ],
   "source": [
    "a = '\\t'+'aaa\\t'+'bbb' +'\\t'\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
